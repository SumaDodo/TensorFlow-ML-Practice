{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stochastic Gradient Descent",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The main demerit of gradient is the batch. Batch is the number of samples used to calculate gradient in an iteration. Sometimes, batch can be an entire dataset. This can be computationally ineffecient.  \n",
        "Thus, stochastic gradient descent goes by the idea of right gradient on an average with much less computation. Randomly chosen single example for each iteration. One down side of this method is the randomness which leads to noise.  \n",
        "While gradient descent is very slow due to its computational inefficiencies, Stochastic gradient descent is much faster and proves improved convergence."
      ],
      "metadata": {
        "id": "lvmrqLJctwaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **How to solve the problem | How to change weights** trainer - optimizer. "
      ],
      "metadata": {
        "id": "fvXTx09twftL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kdCv61Gq2ES"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def stochastic_gradient_descent( gradient, x , y, start, lr=0.1, batch_size=1, n_iter=50, tolerance=1e-06, dtype=\"float64\", random_state=None):\n",
        "\n",
        "  dtype_ = np.dtype(dtype) #data type for numpy arrays\n",
        "\n",
        "  seed = None if random_state is None else int(random_state)\n",
        "  rng = np.random.default_rng(seed=seed) #random number generator\n",
        "\n",
        "  x, y = np.array(x, dtype = dtype_), np.array(y, dtype = dtype_)\n",
        "  xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
        "  number_of_observations = x.shape[0]\n",
        "  if number_of_observations != y.shape[0]:\n",
        "    raise ValueError(\"x and y lengths don't match\")\n",
        "\n",
        "  vector = np.array(start, dtype= dtype_) #initializing the values of variables\n",
        "\n",
        "  learn_rate = np.array(lr, dtype = dtype_)\n",
        "  batch_size = int(batch_size)\n",
        "\n",
        "  #gradient descent loop\n",
        "  for _ in range(n_iter):\n",
        "    rng.shuffle(xy)\n",
        "\n",
        "    for start in range(0, number_of_observations, batch_size): #minibatch\n",
        "      stop = start + batch_size\n",
        "      x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
        "\n",
        "      grad = np.array(stochastic_gradient_descent(x_batch, y_batch, vector), dtype_)\n",
        "      diff = -learn_rate*grad\n",
        "\n",
        "      vector += diff\n",
        "\n",
        "    return vector if vector.shape else vector.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*References*   \n",
        "1. https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent\n",
        "2. https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent  \n",
        "3. https://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent  \n",
        "4. https://realpython.com/gradient-descent-algorithm-python/#:~:text=Stochastic%20gradient%20descent%20is%20an,used%20in%20machine%20learning%20applications."
      ],
      "metadata": {
        "id": "h4W5uMtjyN0W"
      }
    }
  ]
}